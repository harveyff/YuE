#!/usr/bin/env python3
"""
YuE Gradio WebUI - Reference to YuE-UI by joeljuvel
A comprehensive Gradio interface for YuE music generation
"""

import os
import json
import subprocess
import tempfile
from pathlib import Path
from typing import Optional, Tuple, List
import gradio as gr

# Configuration from environment variables
STAGE1_MODEL = os.getenv("STAGE1_MODEL", "m-a-p/YuE-s1-7B-anneal-en-icl")
STAGE2_MODEL = os.getenv("STAGE2_MODEL", "m-a-p/YuE-s2-1B-general")
CUDA_IDX = os.getenv("CUDA_IDX", "0")
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "/app/output")
INFERENCE_SCRIPT = "/app/inference/infer.py"

# Load top 200 tags if available
TOP_TAGS = []
try:
    tags_file = Path(__file__).parent / "top_200_tags.json"
    if tags_file.exists():
        with open(tags_file, 'r', encoding='utf-8') as f:
            TOP_TAGS = json.load(f)
except:
    pass

def run_inference(
    genre_txt: str,
    lyrics_txt: str,
    run_n_segments: int,
    stage2_batch_size: int,
    max_new_tokens: int,
    repetition_penalty: float,
    use_audio_prompt: bool,
    audio_prompt_path: Optional[str],
    prompt_start_time: float,
    prompt_end_time: float,
    use_dual_tracks_prompt: bool,
    vocal_track_prompt_path: Optional[str],
    instrumental_track_prompt_path: Optional[str],
    stage1_model: str,
    stage2_model: str,
    progress=None,
) -> Tuple[str, str]:
    """
    Run YuE inference and return output audio path and status message
    """
    try:
        # Prepare inference command
        cmd = [
            "python", INFERENCE_SCRIPT,
            "--cuda_idx", CUDA_IDX,
            "--stage1_model", stage1_model,
            "--stage2_model", stage2_model,
            "--run_n_segments", str(run_n_segments),
            "--stage2_batch_size", str(stage2_batch_size),
            "--output_dir", OUTPUT_DIR,
            "--max_new_tokens", str(max_new_tokens),
            "--repetition_penalty", str(repetition_penalty),
        ]
        
        # Handle genre and lyrics
        if genre_txt:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                f.write(genre_txt)
                cmd.extend(["--genre_txt", f.name])
        
        if lyrics_txt:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                f.write(lyrics_txt)
                cmd.extend(["--lyrics_txt", f.name])
        
        # Handle audio prompts
        if use_dual_tracks_prompt:
            cmd.append("--use_dual_tracks_prompt")
            if vocal_track_prompt_path:
                cmd.extend(["--vocal_track_prompt_path", vocal_track_prompt_path])
            if instrumental_track_prompt_path:
                cmd.extend(["--instrumental_track_prompt_path", instrumental_track_prompt_path])
            if prompt_start_time is not None:
                cmd.extend(["--prompt_start_time", str(prompt_start_time)])
            if prompt_end_time is not None:
                cmd.extend(["--prompt_end_time", str(prompt_end_time)])
        elif use_audio_prompt:
            cmd.append("--use_audio_prompt")
            if audio_prompt_path:
                cmd.extend(["--audio_prompt_path", audio_prompt_path])
            if prompt_start_time is not None:
                cmd.extend(["--prompt_start_time", str(prompt_start_time)])
            if prompt_end_time is not None:
                cmd.extend(["--prompt_end_time", str(prompt_end_time)])
        
        # Run inference with progress tracking
        if progress:
            progress(0, desc="Starting inference...")
            progress(0.3, desc="Running Stage 1...")
        
        result = subprocess.run(cmd, capture_output=True, text=True, cwd="/app")
        
        if progress:
            progress(0.7, desc="Running Stage 2...")
        
        if result.returncode != 0:
            return None, f"Error: {result.stderr}\n\nCommand: {' '.join(cmd)}"
        
        if progress:
            progress(0.9, desc="Finding output files...")
        
        # Find generated audio files
        output_path = Path(OUTPUT_DIR)
        if not output_path.exists():
            return None, f"Output directory {OUTPUT_DIR} does not exist."
        
        audio_files = list(output_path.glob("*.mp3")) + list(output_path.glob("*.wav"))
        
        if progress:
            progress(1.0, desc="Complete!")
        
        if audio_files:
            latest_file = max(audio_files, key=lambda p: p.stat().st_mtime)
            return str(latest_file), f"âœ… Success! Generated {len(audio_files)} file(s).\nLatest: {latest_file.name}\n\nOutput directory: {OUTPUT_DIR}"
        else:
            return None, f"Generation completed but no audio files found in {OUTPUT_DIR}.\n\nCommand output: {result.stdout}"
            
    except Exception as e:
        return None, f"Exception: {str(e)}"

def create_ui():
    """Create the Gradio UI interface"""
    
    # Note: theme parameter moved to launch() in Gradio 6.0+
    with gr.Blocks(title="YuE Music Generation") as demo:
        gr.Markdown("""
        # ğŸµ YuE Music Generation UI
        **Open Music Foundation Models for Full-Song Generation**
        
        Generate complete songs with vocals and accompaniment from lyrics. Support for multiple languages and music styles.
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ Input Settings")
                
                genre_input = gr.Textbox(
                    label="é£æ ¼æ ‡ç­¾ (Genre Tags)",
                    placeholder="ä¾‹å¦‚: inspiring female uplifting pop airy vocal electronic bright",
                    value="inspiring female uplifting pop airy vocal electronic bright vocal vocal",
                    lines=2,
                    info="æ¨èåŒ…å«ï¼šé£æ ¼ã€ä¹å™¨ã€æƒ…ç»ªã€æ€§åˆ«ã€éŸ³è‰²ï¼Œç”¨ç©ºæ ¼åˆ†éš”"
                )
                
                # Tag suggestions dropdown
                if TOP_TAGS:
                    with gr.Accordion("å¸¸ç”¨æ ‡ç­¾ (Common Tags)", open=False):
                        # Convert to list for Gradio 6.0+ compatibility
                        tag_choices = list(TOP_TAGS[:50]) if isinstance(TOP_TAGS, (list, tuple)) else list(TOP_TAGS)[:50]
                        tag_dropdown = gr.Dropdown(
                            choices=tag_choices,  # Show first 50 tags
                            label="é€‰æ‹©æ ‡ç­¾æ·»åŠ åˆ°è¾“å…¥æ¡†",
                            multiselect=True
                        )
                        def add_tags(selected_tags, current_text):
                            if selected_tags:
                                new_tags = " ".join(selected_tags)
                                return current_text + " " + new_tags if current_text else new_tags
                            return current_text
                        tag_dropdown.change(
                            fn=add_tags,
                            inputs=[tag_dropdown, genre_input],
                            outputs=genre_input
                        )
                
                lyrics_input = gr.Textbox(
                    label="æ­Œè¯ (Lyrics)",
                    placeholder="[verse]\nç¬¬ä¸€æ®µæ­Œè¯...\n\n[chorus]\nå‰¯æ­Œæ­Œè¯...",
                    value="""[verse]
Staring at the sunset, colors paint the sky
Thoughts of you keep swirling, can't deny
I know I let you down, I made mistakes
But I'm here to mend the heart I didn't break

[chorus]
Every road you take, I'll be one step behind
Every dream you chase, I'm reaching for the light
You can't fight this feeling now
I won't back down""",
                    lines=15,
                    info="ä½¿ç”¨ [verse], [chorus], [bridge], [outro] æ ‡ç­¾åˆ†éš”æ®µè½ï¼Œæ®µè½é—´ç”¨ä¸¤ä¸ªæ¢è¡Œç¬¦åˆ†éš”"
                )
                
                with gr.Accordion("ğŸšï¸ Generation Parameters", open=True):
                    run_n_segments = gr.Slider(
                        minimum=1,
                        maximum=10,
                        value=2,
                        step=1,
                        label="æ®µè½æ•°é‡ (Segments)",
                        info="è¦ç”Ÿæˆçš„æ­Œè¯æ®µè½æ•°ã€‚24GBæ˜¾å­˜å»ºè®®â‰¤2ï¼Œ80GBæ˜¾å­˜å¯ç”Ÿæˆå®Œæ•´æ­Œæ›²"
                    )
                    
                    stage2_batch_size = gr.Slider(
                        minimum=1,
                        maximum=16,
                        value=4,
                        step=1,
                        label="Stage2 æ‰¹æ¬¡å¤§å°",
                        info="æ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼Œè¶Šå¤§è¶Šå¿«ä½†å ç”¨æ›´å¤šæ˜¾å­˜"
                    )
                    
                    max_new_tokens = gr.Slider(
                        minimum=1000,
                        maximum=5000,
                        value=3000,
                        step=100,
                        label="æœ€å¤§Tokenæ•°",
                        info="æ¯æ®µçº¦30ç§’éŸ³é¢‘ï¼Œé»˜è®¤3000"
                    )
                    
                    repetition_penalty = gr.Slider(
                        minimum=1.0,
                        maximum=2.0,
                        value=1.1,
                        step=0.1,
                        label="é‡å¤æƒ©ç½š (Repetition Penalty)",
                        info="æ§åˆ¶é‡å¤åº¦ï¼Œ1.1ä¸ºé»˜è®¤å€¼"
                    )
                
                with gr.Accordion("ğŸ§ Audio Prompt / ICL (Optional)", open=False):
                    use_audio_prompt = gr.Checkbox(
                        label="ä½¿ç”¨å•è½¨éŸ³é¢‘æç¤º (Single-track ICL)",
                        value=False
                    )
                    
                    audio_prompt = gr.Audio(
                        label="å‚è€ƒéŸ³é¢‘ (Reference Audio)",
                        type="filepath",
                        visible=False
                    )
                    
                    use_dual_tracks_prompt = gr.Checkbox(
                        label="ä½¿ç”¨åŒè½¨éŸ³é¢‘æç¤º (Dual-track ICL) - æ¨è",
                        value=False
                    )
                    
                    with gr.Row():
                        vocal_track = gr.Audio(
                            label="äººå£°è½¨é“ (Vocal Track)",
                            type="filepath",
                            visible=False
                        )
                        instrumental_track = gr.Audio(
                            label="ä¼´å¥è½¨é“ (Instrumental Track)",
                            type="filepath",
                            visible=False
                        )
                    
                    with gr.Row():
                        prompt_start_time = gr.Number(
                            label="å¼€å§‹æ—¶é—´ (ç§’)",
                            value=0.0,
                            precision=1,
                            visible=False
                        )
                        prompt_end_time = gr.Number(
                            label="ç»“æŸæ—¶é—´ (ç§’)",
                            value=30.0,
                            precision=1,
                            visible=False
                        )
                    
                    # Toggle visibility based on checkbox
                    def toggle_audio_prompt(use_single, use_dual):
                        return (
                            gr.update(visible=use_single),
                            gr.update(visible=use_dual),
                            gr.update(visible=use_dual),
                            gr.update(visible=use_single or use_dual),
                            gr.update(visible=use_single or use_dual)
                        )
                    
                    use_audio_prompt.change(
                        fn=lambda x: toggle_audio_prompt(x, False),
                        inputs=[use_audio_prompt],
                        outputs=[audio_prompt, vocal_track, instrumental_track, prompt_start_time, prompt_end_time]
                    )
                    
                    use_dual_tracks_prompt.change(
                        fn=lambda x: toggle_audio_prompt(False, x),
                        inputs=[use_dual_tracks_prompt],
                        outputs=[audio_prompt, vocal_track, instrumental_track, prompt_start_time, prompt_end_time]
                    )
                
                with gr.Accordion("ğŸ¤– Model Selection", open=False):
                    stage1_model = gr.Dropdown(
                        choices=[
                            "m-a-p/YuE-s1-7B-anneal-en-icl",
                            "m-a-p/YuE-s1-7B-anneal-en-cot",
                            "m-a-p/YuE-s1-7B-anneal-zh-icl",
                            "m-a-p/YuE-s1-7B-anneal-zh-cot",
                            "m-a-p/YuE-s1-7B-anneal-jp-kr-icl",
                            "m-a-p/YuE-s1-7B-anneal-jp-kr-cot",
                        ],
                        value=STAGE1_MODEL,
                        label="Stage 1 Model",
                        info="ICLæ¨¡å‹æ”¯æŒéŸ³é¢‘æç¤ºï¼ŒCoTæ¨¡å‹ä¸ºé“¾å¼æ€è€ƒæ¨¡å¼"
                    )
                    
                    stage2_model = gr.Dropdown(
                        choices=[
                            "m-a-p/YuE-s2-1B-general",
                        ],
                        value=STAGE2_MODEL,
                        label="Stage 2 Model",
                        info="Stage 2ç”¨äºéŸ³é¢‘ç²¾ç‚¼"
                    )
                
                generate_btn = gr.Button("ğŸµ ç”ŸæˆéŸ³ä¹ (Generate Music)", variant="primary", size="lg")
            
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ¼ Output")
                
                output_audio = gr.Audio(
                    label="ç”Ÿæˆçš„éŸ³ä¹ (Generated Music)",
                    type="filepath"
                )
                
                status_output = gr.Textbox(
                    label="çŠ¶æ€ä¿¡æ¯ (Status)",
                    lines=5,
                    interactive=False
                )
                
                gr.Markdown("""
                ### ğŸ’¡ Tips & Tricks
                
                - **é£æ ¼æ ‡ç­¾**: æ¨èåŒ…å«5ä¸ªè¦ç´ ï¼šé£æ ¼ã€ä¹å™¨ã€æƒ…ç»ªã€æ€§åˆ«ã€éŸ³è‰²
                - **æ­Œè¯æ ¼å¼**: ä½¿ç”¨ `[verse]`, `[chorus]`, `[bridge]`, `[outro]` æ ‡ç­¾
                - **éŸ³é¢‘æç¤º**: åŒè½¨æ¨¡å¼æ•ˆæœæœ€å¥½ï¼Œéœ€è¦åˆ†ç¦»äººå£°å’Œä¼´å¥
                - **æ˜¾å­˜è¦æ±‚**: 
                  - 24GBæ˜¾å­˜ï¼šå»ºè®®â‰¤2ä¸ªæ®µè½
                  - 80GBæ˜¾å­˜ï¼šå¯ç”Ÿæˆå®Œæ•´æ­Œæ›²ï¼ˆ4+æ®µè½ï¼‰
                - **ç”Ÿæˆæ—¶é—´**: 
                  - RTX 4090: ~360ç§’/30ç§’éŸ³é¢‘
                  - H800: ~150ç§’/30ç§’éŸ³é¢‘
                
                ### ğŸ“š Resources
                - [YuE Official Repo](https://github.com/multimodal-art-projection/YuE)
                - [YuE-UI by joeljuvel](https://github.com/joeljuvel/YuE-UI)
                - [Paper](https://arxiv.org/abs/2503.08638)
                """)
        
        # Generation function wrapper
        def generate_wrapper(
            genre, lyrics, segments, batch_size, max_tokens, rep_penalty,
            use_audio, audio_path, start_time, end_time,
            use_dual, vocal_path, inst_path, s1_model, s2_model,
            progress=gr.Progress()
        ):
            # Extract file paths from Gradio Audio components
            # Gradio Audio returns a tuple (file_path, sample_rate) or just file_path
            if isinstance(audio_path, tuple):
                audio_file_path = audio_path[0] if use_audio and audio_path[0] else None
            else:
                audio_file_path = audio_path if use_audio and audio_path else None
            
            if isinstance(vocal_path, tuple):
                vocal_file_path = vocal_path[0] if use_dual and vocal_path[0] else None
            else:
                vocal_file_path = vocal_path if use_dual and vocal_path else None
            
            if isinstance(inst_path, tuple):
                inst_file_path = inst_path[0] if use_dual and inst_path[0] else None
            else:
                inst_file_path = inst_path if use_dual and inst_path else None
            
            audio_output, status = run_inference(
                genre_txt=genre or "",
                lyrics_txt=lyrics or "",
                run_n_segments=int(segments),
                stage2_batch_size=int(batch_size),
                max_new_tokens=int(max_tokens),
                repetition_penalty=float(rep_penalty),
                use_audio_prompt=use_audio,
                audio_prompt_path=audio_file_path,
                prompt_start_time=float(start_time) if start_time else 0.0,
                prompt_end_time=float(end_time) if end_time else 30.0,
                use_dual_tracks_prompt=use_dual,
                vocal_track_prompt_path=vocal_file_path,
                instrumental_track_prompt_path=inst_file_path,
                stage1_model=s1_model,
                stage2_model=s2_model,
                progress=progress
            )
            
            return audio_output, status
        
        generate_btn.click(
            fn=generate_wrapper,
            inputs=[
                genre_input, lyrics_input, run_n_segments, stage2_batch_size,
                max_new_tokens, repetition_penalty,
                use_audio_prompt, audio_prompt, prompt_start_time, prompt_end_time,
                use_dual_tracks_prompt, vocal_track, instrumental_track,
                stage1_model, stage2_model
            ],
            outputs=[output_audio, status_output]
        )
        
        gr.Markdown("""
        ---
        **YuE Music Generation UI** - Based on [YuE-UI](https://github.com/joeljuvel/YuE-UI) design
        """)
    
    return demo

if __name__ == "__main__":
    demo = create_ui()
    demo.launch(
        server_name=os.getenv("GRADIO_SERVER_NAME", "0.0.0.0"),
        server_port=int(os.getenv("GRADIO_SERVER_PORT", "7860")),
        theme=gr.themes.Soft(),  # Theme moved here in Gradio 6.0+
        share=False
    )

